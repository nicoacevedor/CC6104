# 1. Visualizar la correlación de los datos
correlation_data <- cor(my.frame)
print(correlation_data )
cat("\n")
# 2. Visualizar la correlación de los datos
correlation_var <- cor(sat_scores, gpa_scores)
cat("Correlación entre SAT y GPA:", correlation_var, "\n")
# 3. Metodo bootstrap para calcular el error estandar de la correlacion
myboot<-function(x,y,fun,nRuns,sampleSize,alpha){
values<-vector()
for(i in 1:nRuns){
samp.i<-sample(x, size = sampleSize, replace = T)
samp.j<-sample(y, size = sampleSize, replace = T)
values[i]<-fun(samp.i,samp.j)
}
point.est <-fun(x,y)
se <- sd(values)
hist(values,main = "Histograma de correlaciones", xlab="Correlacion",ylab = "Frecuencia")
l.CI <- quantile(values, alpha/2)
u.CI <- quantile(values, 1-alpha/2)
return(c("Point Estimate"=point.est,
"Standard error"=se,
"Lower CI limit" = l.CI,
"Upper CI limit" = u.CI))
}
cat("\n")
myboot(sat_scores,gpa_scores,cor,5000,100,0.05)
my.frame <- read.table(file = "SAT_GPA.csv",header = T,sep = ",", fileEncoding = "latin1")
# Obtener los valores de las variables SAT y GPA
sat_scores <- my.frame$Total
gpa_scores <- my.frame$Gpa
# 1. Visualizar la correlación de los datos
correlation_data <- cor(my.frame)
print(correlation_data)
cat("\n")
# 2. Visualizar la correlación de los datos
correlation_var <- cor(sat_scores, gpa_scores)
cat("Correlación entre SAT y GPA:", correlation_var, "\n")
# 3. Metodo bootstrap para calcular el error estandar de la correlacion
myboot<-function(x,y,fun,nRuns,sampleSize,alpha){
values<-vector()
for(i in 1:nRuns){
samp.i<-sample(x, size = sampleSize, replace = T)
samp.j<-sample(y, size = sampleSize, replace = T)
values[i]<-fun(samp.i,samp.j)
}
point.est <-fun(x,y)
# error estandar
se <- sd(values)
# 4. Visualizar el grafico de Histograma
hist(values,main = "Histograma de correlaciones", xlab="Correlacion",ylab = "Frecuencia")
#5. Obtener el 95% de intervalo de confianza de la estimación por cuantiles
l.CI <- quantile(values, alpha/2)
u.CI <- quantile(values, 1-alpha/2)
return(c("Point Estimate"=point.est,
"Standard error"=se,
"Lower CI limit" = l.CI,
"Upper CI limit" = u.CI))
}
cat("\n")
myboot(sat_scores,gpa_scores,cor,5000,100,0.05)
# Manipulación de estructuras
library(tidyverse)
# Para realizar plots
library(ggplot2)
library(plotly)
# Manipulación de varios plots en una imagen.
library(gridExtra)
# librería para bootstrap
library(boot)
n_max <- 50000
p <- 0.5
# Vectores del estimador y de la distribución de Bernoulli
estimator <- vector(length = n_max)
bernoulli <- rbinom(n_max, size = 1, prob = p)
# Para cada valor del vector, se promedia con los anteriores
# y se agregan al estimador
for (k in 1:n_max) {
estimator[k] <- sum(bernoulli[1:k]) / k
}
# Gráfico de los experimentos
plot_est <- plot_ly(
x = 1:n_max,
y = estimator,
mode = "lines",
type = "scatter",
name = "Experimentos",
line = list(color = "blue")
) %>%
add_trace(
x = 1:n_max,
y = 0.5,
name = "p = 0.5",
line = list(color = "red")
) %>%
layout(
title = "Estimador Pn para X ~ Bernoulli(p) con p = 0.5",
xaxis = list(title = sprintf("Pasos (%d)", n_max)),
yaxis = list(title = "Probabilidad")
)
plot_est
n_max <- 50000
estimator <- vector(length = n_max)
bernoulli <- rbinom(n_max, size = 1, prob = 0.5)
noise <- rnorm(n_max)
T <- vector(length = n_max)
for (k in 1:n_max) {
estimator[k] <- sum(bernoulli[1:k]) / k
T[k] = estimator[k] + noise[k]
}
plot_est <- plot_ly(
x = 1:n_max,
y = T,
mode = "lines",
type = "scatter",
name = "Experimentos",
line = list(color = "blue")
) %>%
add_trace(
x = 1:n_max,
y = 0.5,
name = "p = 0.5",
line = list(color = "red")
) %>%
layout(
title = "Estimador Tn para X ~ Bernoulli(p) con p = 0.5",
xaxis = list(title = sprintf("Pasos (%d)", n_max)),
yaxis = list(title = "Probabilidad")
)
plot_est
plot_est <- plot_ly(
x = 1:n_max,
y = noise,
mode = "lines",
type = "scatter",
name = "Ruido",
line = list(color = "blue")
) %>%
layout(
title = "Ruido e ~ N(0, 1)",
xaxis = list(title = sprintf("Pasos (%d)", n_max)),
yaxis = list(title = "Probabilidad")
)
plot_est
data_raw <- read.csv("Body Measurements _ original.csv")
totalheight_plot <- ggplot(
data = data_raw,
aes(x = TotalHeight, color = "density")
)
totalheight_dens <- geom_density(color = "#67B7D1")
totalheight_hist <- geom_histogram(
aes(y = after_stat(density)),
bins = 10,
fill = "#67B7D1",
alpha = 0.5
)
final_plot <- totalheight_plot + totalheight_dens + totalheight_hist +
ylab("") + xlab("") + scale_color_manual(values = c("density" = "#67B7D1"))
ggplotly(final_plot) %>%
layout(
xaxis = list(title = "TotalHeight"),
yaxis = list(title = "Frequency")
)
likelihood <- function(mu, sigma, sub = NULL) {
x_org <- data_raw$TotalHeight
if (is.null(sub))
x <- x_org
else
x <- sample(x_org, sub)
n <- length(x)
x_mean <- mean(x)
x_var <- var(x)
first_sum <- (n * x_var) / (2 * sigma^2)
second_sum <- (n * (x_mean - mu)^2) / (2 * sigma^2)
-n * log(sigma) - first_sum - second_sum
}
mu <- seq(from = 20, to = 80, by = 0.5)
sigma <- seq(from = 5, to = 23, by = 0.5)
ll_plot <- Vectorize(likelihood)
ll_plot <- outer(X = mu, Y = sigma, ll_plot)
filled.contour(x = mu, y = sigma, z = ll_plot,
xlab = expression(mu), ylab = expression(sigma))
sigma <- 12
p <- plot_ly(x = mu, y = likelihood(mu, sigma, 100),
type = "scatter", name = "100 datos")
p <- add_trace(p, y = likelihood(mu, sigma, 300), name = "300 datos")
p <- add_trace(p, y = likelihood(mu, sigma), name = "Todos los datos")
p <- p %>% layout(title = "Likelihood para valores de mu y sigma = 12",
xaxis = list(title = "mu"),
yaxis = list(title = "valor de likelihood")
)
p
# Primero, redefinimos la función likelihood para aplicar nlminb
likelihood <- function(param) {
mu <- param[1]
sigma <- param[2]
x <- data_raw$TotalHeight
n <- length(x)
x_mean <- mean(x)
x_var <- var(x)
first_sum <- (n * x_var) / (2 * sigma^2)
second_sum <- (n * (x_mean - mu)^2) / (2 * sigma^2)
n * log(sigma) + first_sum + second_sum
}
# Aplicamos nlminb
MLE <- nlminb(
objective = likelihood,
start = c(10, 10),
lower = c(0.001, 0.001),
upper = c(500, 500)
)
cat("mu = ", MLE$par[1], ", sigma = ", MLE$par[2])
data_plot <- ggplot(
data = data_raw,
aes(x = ShoulderToWaist, color = "density")
)
data_dens <- geom_density(color = "#67B7D1")
data_hist <- geom_histogram(
aes(y = after_stat(density)),
bins = 10,
fill = "#67B7D1",
alpha = 0.5
)
final_plot <- data_plot + data_dens + data_hist +
ylab("") + xlab("") + scale_color_manual(values = c("density" = "#67B7D1"))
ggplotly(final_plot) %>%
layout(
xaxis = list(title = "ShoulderToWaist"),
yaxis = list(title = "Frequency")
)
likelihood <- function(mu, sigma) {
x <- data_raw$ShoulderToWaist
sum(log(dnorm(x, mean = mu, sd = sigma)))
}
mu <- seq(from = 10, to = 30, by = 0.5)
sigma <- seq(from = 3, to = 10, by = 0.5)
ll_plot <- Vectorize(likelihood)
ll_plot <- outer(X = mu, Y = sigma, ll_plot)
filled.contour(x = mu, y = sigma, z = ll_plot,
xlab = expression(mu), ylab = expression(sigma))
# Se redefine la función likelihood para usar nlminb
likelihood <- function(param) {
mu <- param[1]
sigma <- param[2]
x <- data_raw$ShoulderToWaist
-sum(log(dnorm(x, mean = mu, sd = sigma)))
}
MLE <- nlminb(objective = likelihood, start = c(10, 10), lower = c(0.001, 0.001), upper = c(500, 500))
cat("mu = ", MLE$par[1], ", sigma = ", MLE$par[2])
my.frame <- read.table(file = "SAT_GPA.csv",header = T,sep = ",", fileEncoding = "latin1")
my.frame <- read.table(file = "SAT_GPA.csv",header = T,sep = ",", fileEncoding = "latin1")
my.frame <- read.table(file = "SAT_GPA.csv",header = T,sep = ",", fileEncoding = "latin1")
# Manipulación de estructuras
library(tidyverse)
# Para realizar plots
library(ggplot2)
library(plotly)
# Manipulación de varios plots en una imagen.
library(gridExtra)
# librería para bootstrap
library(boot)
n_max <- 50000
p <- 0.5
# Vectores del estimador y de la distribución de Bernoulli
estimator <- vector(length = n_max)
bernoulli <- rbinom(n_max, size = 1, prob = p)
# Para cada valor del vector, se promedia con los anteriores
# y se agregan al estimador
for (k in 1:n_max) {
estimator[k] <- sum(bernoulli[1:k]) / k
}
# Gráfico de los experimentos
plot_est <- plot_ly(
x = 1:n_max,
y = estimator,
mode = "lines",
type = "scatter",
name = "Experimentos",
line = list(color = "blue")
) %>%
add_trace(
x = 1:n_max,
y = 0.5,
name = "p = 0.5",
line = list(color = "red")
) %>%
layout(
title = "Estimador Pn para X ~ Bernoulli(p) con p = 0.5",
xaxis = list(title = sprintf("Pasos (%d)", n_max)),
yaxis = list(title = "Probabilidad")
)
plot_est
n_max <- 50000
estimator <- vector(length = n_max)
bernoulli <- rbinom(n_max, size = 1, prob = 0.5)
noise <- rnorm(n_max)
T <- vector(length = n_max)
for (k in 1:n_max) {
estimator[k] <- sum(bernoulli[1:k]) / k
T[k] = estimator[k] + noise[k]
}
plot_est <- plot_ly(
x = 1:n_max,
y = T,
mode = "lines",
type = "scatter",
name = "Experimentos",
line = list(color = "blue")
) %>%
add_trace(
x = 1:n_max,
y = 0.5,
name = "p = 0.5",
line = list(color = "red")
) %>%
layout(
title = "Estimador Tn para X ~ Bernoulli(p) con p = 0.5",
xaxis = list(title = sprintf("Pasos (%d)", n_max)),
yaxis = list(title = "Probabilidad")
)
plot_est
plot_est <- plot_ly(
x = 1:n_max,
y = noise,
mode = "lines",
type = "scatter",
name = "Ruido",
line = list(color = "blue")
) %>%
layout(
title = "Ruido e ~ N(0, 1)",
xaxis = list(title = sprintf("Pasos (%d)", n_max)),
yaxis = list(title = "Probabilidad")
)
plot_est
data_raw <- read.csv("Body Measurements _ original.csv")
totalheight_plot <- ggplot(
data = data_raw,
aes(x = TotalHeight, color = "density")
)
totalheight_dens <- geom_density(color = "#67B7D1")
totalheight_hist <- geom_histogram(
aes(y = after_stat(density)),
bins = 10,
fill = "#67B7D1",
alpha = 0.5
)
final_plot <- totalheight_plot + totalheight_dens + totalheight_hist +
ylab("") + xlab("") + scale_color_manual(values = c("density" = "#67B7D1"))
ggplotly(final_plot) %>%
layout(
xaxis = list(title = "TotalHeight"),
yaxis = list(title = "Frequency")
)
likelihood <- function(mu, sigma, sub = NULL) {
x_org <- data_raw$TotalHeight
if (is.null(sub))
x <- x_org
else
x <- sample(x_org, sub)
n <- length(x)
x_mean <- mean(x)
x_var <- var(x)
first_sum <- (n * x_var) / (2 * sigma^2)
second_sum <- (n * (x_mean - mu)^2) / (2 * sigma^2)
-n * log(sigma) - first_sum - second_sum
}
mu <- seq(from = 20, to = 80, by = 0.5)
sigma <- seq(from = 5, to = 23, by = 0.5)
ll_plot <- Vectorize(likelihood)
ll_plot <- outer(X = mu, Y = sigma, ll_plot)
filled.contour(x = mu, y = sigma, z = ll_plot,
xlab = expression(mu), ylab = expression(sigma))
sigma <- 12
p <- plot_ly(x = mu, y = likelihood(mu, sigma, 100),
type = "scatter", name = "100 datos")
p <- add_trace(p, y = likelihood(mu, sigma, 300), name = "300 datos")
p <- add_trace(p, y = likelihood(mu, sigma), name = "Todos los datos")
p <- p %>% layout(title = "Likelihood para valores de mu y sigma = 12",
xaxis = list(title = "mu"),
yaxis = list(title = "valor de likelihood")
)
p
# Primero, redefinimos la función likelihood para aplicar nlminb
likelihood <- function(param) {
mu <- param[1]
sigma <- param[2]
x <- data_raw$TotalHeight
n <- length(x)
x_mean <- mean(x)
x_var <- var(x)
first_sum <- (n * x_var) / (2 * sigma^2)
second_sum <- (n * (x_mean - mu)^2) / (2 * sigma^2)
n * log(sigma) + first_sum + second_sum
}
# Aplicamos nlminb
MLE <- nlminb(
objective = likelihood,
start = c(10, 10),
lower = c(0.001, 0.001),
upper = c(500, 500)
)
cat("mu = ", MLE$par[1], ", sigma = ", MLE$par[2])
data_plot <- ggplot(
data = data_raw,
aes(x = ShoulderToWaist, color = "density")
)
data_dens <- geom_density(color = "#67B7D1")
data_hist <- geom_histogram(
aes(y = after_stat(density)),
bins = 10,
fill = "#67B7D1",
alpha = 0.5
)
final_plot <- data_plot + data_dens + data_hist +
ylab("") + xlab("") + scale_color_manual(values = c("density" = "#67B7D1"))
ggplotly(final_plot) %>%
layout(
xaxis = list(title = "ShoulderToWaist"),
yaxis = list(title = "Frequency")
)
likelihood <- function(mu, sigma) {
x <- data_raw$ShoulderToWaist
sum(log(dnorm(x, mean = mu, sd = sigma)))
}
mu <- seq(from = 10, to = 30, by = 0.5)
sigma <- seq(from = 3, to = 10, by = 0.5)
ll_plot <- Vectorize(likelihood)
ll_plot <- outer(X = mu, Y = sigma, ll_plot)
filled.contour(x = mu, y = sigma, z = ll_plot,
xlab = expression(mu), ylab = expression(sigma))
# Se redefine la función likelihood para usar nlminb
likelihood <- function(param) {
mu <- param[1]
sigma <- param[2]
x <- data_raw$ShoulderToWaist
-sum(log(dnorm(x, mean = mu, sd = sigma)))
}
MLE <- nlminb(objective = likelihood, start = c(10, 10), lower = c(0.001, 0.001), upper = c(500, 500))
cat("mu = ", MLE$par[1], ", sigma = ", MLE$par[2])
my.frame <- read.table(file = "SAT_GPA.csv",header = T,sep = ",", fileEncoding = "latin1")
my.frame <- read.table(file = "SAT_GPA.csv",header = T,sep = ",", fileEncoding = "latin1")
my.frame <- read.table(file = "SAT_GPA.csv",header = T,sep = ",", fileEncoding = "latin1")
my.frame <- read.table(file = "SAT_GPA.csv",header = T,sep = ",", fileEncoding = "latin1")
my.frame <- read.table(file = "SAT_GPA.csv",header = TRUE,sep = ",", fileEncoding = "latin1")
# Obtener los valores de las variables SAT y GPA
sat_scores <- my.frame$Total
gpa_scores <- my.frame$Gpa
# 1. Visualizar la correlación de los datos
correlation_data <- cor(my.frame)
print(correlation_data)
cat("\n")
# 2. Visualizar la correlación de los datos
correlation_var <- cor(sat_scores, gpa_scores)
cat("Correlación entre SAT y GPA:", correlation_var, "\n")
# 3. Metodo bootstrap para calcular el error estandar de la correlacion
myboot<-function(x,y,fun,nRuns,sampleSize,alpha){
values<-vector()
for(i in 1:nRuns){
samp.i<-sample(x, size = sampleSize, replace = T)
samp.j<-sample(y, size = sampleSize, replace = T)
values[i]<-fun(samp.i,samp.j)
}
point.est <-fun(x,y)
# error estandar
se <- sd(values)
# 4. Visualizar el grafico de Histograma
hist(values,main = "Histograma de correlaciones", xlab="Correlacion",ylab = "Frecuencia")
#5. Obtener el 95% de intervalo de confianza de la estimación por cuantiles
l.CI <- quantile(values, alpha/2)
u.CI <- quantile(values, 1-alpha/2)
return(c("Point Estimate"=point.est,
"Standard error"=se,
"Lower CI limit" = l.CI,
"Upper CI limit" = u.CI))
}
cat("\n")
myboot(sat_scores,gpa_scores,cor,5000,100,0.05)
my.frame <- read.table(file = "SAT_GPA.csv",header = TRUE,sep = ",", fileEncoding = "latin1")
# Obtener los valores de las variables SAT y GPA
sat_scores <- my.frame$Total
gpa_scores <- my.frame$Gpa
# 1. Visualizar la correlación de los datos
correlation_data <- cor(my.frame)
print(correlation_data)
cat("\n")
# 2. Visualizar la correlación de los datos
correlation_var <- cor(sat_scores, gpa_scores)
cat("Correlación entre SAT y GPA:", correlation_var, "\n")
# 3. Metodo bootstrap para calcular el error estandar de la correlacion
myboot<-function(x,y,fun,nRuns,sampleSize,alpha){
values<-vector()
for(i in 1:nRuns){
samp.i<-sample(x, size = sampleSize, replace = TRUE)
samp.j<-sample(y, size = sampleSize, replace = TRUE)
values[i]<-fun(samp.i,samp.j)
}
point.est <-fun(x,y)
# error estandar
se <- sd(values)
# 4. Visualizar el grafico de Histograma
hist(values,main = "Histograma de correlaciones", xlab="Correlacion",ylab = "Frecuencia")
#5. Obtener el 95% de intervalo de confianza de la estimación por cuantiles
l.CI <- quantile(values, alpha/2)
u.CI <- quantile(values, 1-alpha/2)
return(c("Point Estimate"=point.est,
"Standard error"=se,
"Lower CI limit" = l.CI,
"Upper CI limit" = u.CI))
}
cat("\n")
myboot(sat_scores,gpa_scores,cor,5000,100,0.05)
